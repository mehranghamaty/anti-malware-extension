{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "datasetpath = '../archive/'\n",
    "training_data = datasetpath + \"train/\"\n",
    "test_data = datasetpath + \"test/\"\n",
    "image_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48422 files belonging to 10 classes.\n",
      "Found 3458 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory= training_data,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(image_size, image_size))\n",
    "validation_ds = keras.utils.image_dataset_from_directory(\n",
    "    directory= test_data,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=32,\n",
    "    image_size=(image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 71x71; Received: input_shape=(32, 32, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mapplications\u001b[39m.\u001b[39;49mXception(\n\u001b[0;32m      2\u001b[0m     weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, input_shape\u001b[39m=\u001b[39;49m(image_size, image_size, \u001b[39m3\u001b[39;49m), classes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m model\u001b[39m.\u001b[39mfit(train_ds, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, validation_data\u001b[39m=\u001b[39mvalidation_ds)\n",
      "File \u001b[1;32mc:\\Users\\mehra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\applications\\xception.py:136\u001b[0m, in \u001b[0;36mXception\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    131\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIf using `weights` as `\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m` with `include_top`\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m as true, `classes` should be 1000\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     )\n\u001b[0;32m    135\u001b[0m \u001b[39m# Determine proper input shape\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m input_shape \u001b[39m=\u001b[39m imagenet_utils\u001b[39m.\u001b[39;49mobtain_input_shape(\n\u001b[0;32m    137\u001b[0m     input_shape,\n\u001b[0;32m    138\u001b[0m     default_size\u001b[39m=\u001b[39;49m\u001b[39m299\u001b[39;49m,\n\u001b[0;32m    139\u001b[0m     min_size\u001b[39m=\u001b[39;49m\u001b[39m71\u001b[39;49m,\n\u001b[0;32m    140\u001b[0m     data_format\u001b[39m=\u001b[39;49mbackend\u001b[39m.\u001b[39;49mimage_data_format(),\n\u001b[0;32m    141\u001b[0m     require_flatten\u001b[39m=\u001b[39;49minclude_top,\n\u001b[0;32m    142\u001b[0m     weights\u001b[39m=\u001b[39;49mweights,\n\u001b[0;32m    143\u001b[0m )\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m input_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     img_input \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n",
      "File \u001b[1;32mc:\\Users\\mehra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\applications\\imagenet_utils.py:408\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    401\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    402\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mThe input must have 3 channels; Received \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`input_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m                 )\n\u001b[0;32m    405\u001b[0m             \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    406\u001b[0m                 input_shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m0\u001b[39m] \u001b[39m<\u001b[39m min_size\n\u001b[0;32m    407\u001b[0m             ) \u001b[39mor\u001b[39;00m (input_shape[\u001b[39m1\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m input_shape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m min_size):\n\u001b[1;32m--> 408\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    409\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mInput size must be at least \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39mx\u001b[39m\u001b[39m{\u001b[39;00mmin_size\u001b[39m}\u001b[39;00m\u001b[39m; Received: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_shape=\u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m                 )\n\u001b[0;32m    413\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m require_flatten:\n",
      "\u001b[1;31mValueError\u001b[0m: Input size must be at least 71x71; Received: input_shape=(32, 32, 3)"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.applications.MobileNet(\n",
    "    weights=None, input_shape=(image_size, image_size, 3), classes=10)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(train_ds, epochs=10, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c55ea8b3944439389f35f2ec00b6accf74c4fd7bb00fbc72d904351f699e763c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
