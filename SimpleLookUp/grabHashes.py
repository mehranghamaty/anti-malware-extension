import requests
import lxml
from bs4 import BeautifulSoup
import urllib.request
import os

downloadFolder = "./hashes/"
cleanedFolder = "./hashesCleaned/" 
base_url = "https://virusshare.com/"
hash_list_url = base_url + "hashes"
headers = {
  'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'
}



f = requests.get(hash_list_url, headers = headers)
hash_lst = []
soup = BeautifulSoup(f.content, 'lxml')
links = soup.find('table').find_all('a')
num = 0
for anchor in links:
    link = anchor['href']
    if link[-3:] == "md5":
        hash_lst.append(base_url+'/'+link);

def makeFolderIfNotExists(folderPath):
    if not os.path.exists(folderPath):
        os.makedirs(folderPath)

makeFolderIfNotExists(downloadFolder)
makeFolderIfNotExists(cleanedFolder)

for download_url in hash_lst:
    filename = download_url.split('/')[-1]
    print(download_url)
    urllib.request.urlretrieve(download_url, downloadFolder + filename)